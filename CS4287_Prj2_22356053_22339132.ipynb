{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466de51a",
   "metadata": {},
   "source": [
    "## 1: Introduction\n",
    "\n",
    "CS4287 Option 2 -  Atari with extra credit 6% - implement DQN for one of the environments in OpenAI Gym for an Atari game.\n",
    "\n",
    "Names: Christopher Brophy (22356053), Mark Hughes (22339132)\n",
    "Game: Atari Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1925221",
   "metadata": {},
   "source": [
    "## 2: Why Reinforcement Learning is the machine learning paradigm of choice for this task\n",
    "\n",
    "Reinforcement Learning is the correct paradigm for this task because:<br>\n",
    "1) Sequential Desicion Making: \n",
    "    - Breakout is a game where the agent must make a series of decisions over time. Each action influences the next state of the game.\n",
    "    - Reinforcement Learning (RL) handles such sequential decision-making problems because it learns policies that maximise long-term rewards, not just immediate outcomes.\n",
    "2) Delayed Rewards:\n",
    "    - Big amounts of points are only earned when the ball breaks a brick, which happens several steps after the agent moves the paddle.\n",
    "    - RL is designed to learn from delayed rewards, unlike supervised learning.\n",
    "3) Interaction with Environment:\n",
    "    - RL allows the agent to learn from trial and error by interacting with the environment. \n",
    "    - This agent sees the consequences of its actions and adjusts its strategy to improve performance over time.\n",
    "    - Supervised learning would need datasets of ideal, optimal moves, which don't exist.\n",
    "4) No explicit supervision:\n",
    "    - There is no \"correct\" move dataset for every state in breakout.\n",
    "    - RL allows the agent to learn automatically by exploring actions and maximising rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"gymnasium[atari]\" ale-py autorom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8614dd",
   "metadata": {},
   "source": [
    "## 3: The Gym Environment\n",
    "\n",
    "OpenAI Gym provides a standardised interface for reinforcement learning environments.<br>\n",
    "\n",
    "For Atari Breakout:\n",
    "1) Environment Creation\n",
    "    - It initialises the game for me further down with the function \"gym.make('ALE/Breakout-v5')\".\n",
    "    - Handles game mechanics, rendering, and scoring automatically.\n",
    "2) Interactions\n",
    "    - \"obs, reward, terminated, truncated, info = env.step(action)\" lets the agent take actions.\n",
    "        - obs = the next game frame\n",
    "        - reward = points received for the action\n",
    "        - terminated = whether the game is over\n",
    "        - truncated = whether the episode ended due to time\n",
    "        - info = additional info such as lives\n",
    "3) Why it's useful\n",
    "    - Provides a ready-made environment for testing RL agents.\n",
    "    - Base rewards such as breaking bricks is automatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e7e45d",
   "metadata": {},
   "source": [
    "## 4(a): Capture and pre-processing of the data\n",
    "\n",
    "1) Capture: The agent gets raw RGB frames from Gym.\n",
    "2) Preprocessing: \n",
    "    - Frames are converted to grayscale to reduce complexity.\n",
    "    - Resized to 84x84 to standardise input dimensions.\n",
    "    - It is then converted to a numpy array for pytorch to use.\n",
    "3) Reward Shaping:\n",
    "    - In addition to preprocessing, the reward is modified based on structured game features toencourage desirable behaviour.\n",
    "    - Reward shaping provides richer, more consistent feedback, helping the agent converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    img = Image.fromarray(frame).convert('L')\n",
    "    img = img.resize((84, 84), Image.NEAREST)\n",
    "    return np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "def show_frame_inline(frame_rgb, paddle_x=None, reward=None, action=None, q_values=None, scale=5, font_size=22):\n",
    "    # copy frame and resize it\n",
    "    arr = frame_rgb\n",
    "    if arr.dtype == np.float32 or arr.dtype == np.float64:\n",
    "        arr = (np.clip(arr, 0, 1) * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(arr)\n",
    "    img = img.resize((img.width*scale, img.height*scale), Image.NEAREST)\n",
    "\n",
    "    # create context for drawing and font\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    y = 1\n",
    "    spacing = font_size\n",
    "\n",
    "    # draw current reward to gui\n",
    "    if reward is not None:\n",
    "        draw.text((5, y), f\"Reward: {reward:.2f}\", fill=(0,255,0), font=font)\n",
    "        y += spacing\n",
    "    \n",
    "    # draw current paddle x to gui\n",
    "    if paddle_x is not None:\n",
    "        draw.text((5, y), f\"Paddle X: {paddle_x}\", fill=(0,255,0), font=font)\n",
    "        y += spacing\n",
    "\n",
    "    # draw current action to gui\n",
    "    if action is not None:\n",
    "        draw.text((5, y), f\"Action: {action}\", fill=(0,255,0), font=font)\n",
    "        y += spacing\n",
    "\n",
    "    # draw q values to gui\n",
    "    if q_values is not None:\n",
    "        q_str = \", \".join([f\"{v:.2f}\" for v in q_values])\n",
    "        draw.text((5, y), f\"Top Q: {q_str}\", fill=(0,255,0), font=font)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d3ebb",
   "metadata": {},
   "source": [
    "## 4(b): Network Structure\n",
    "\n",
    "1) Input:\n",
    "    - The network takes stacked frames as input with shape (channel, height, width)\n",
    "    - For Breakout, typically 4 frames are stacked to capture motion.\n",
    "2) Convolutional Layers:\n",
    "    - Three convolutional layers exxtract spacial features from the input frames:\n",
    "        1) Conv2d(32, kernel=8, stride=4) => Extracts large spatial patterns (ball, paddle)\n",
    "        2) Conv2d(64, kernel=4, stride=2) => Finer spatial features\n",
    "        3) Conv2d(64, kernel=3, stride=1) => Detailed local features\n",
    "    - ReLU activations after each layer introduce non-linearality.\n",
    "    - These layers convert raw pixel input into a feature representation sutiable for Q-value learning.\n",
    "3) Fully connected layers:\n",
    "    - Flattens the convolutional input into a single vector.\n",
    "    - This flattened vector is passed through:\n",
    "        1) nn.Linear(conv_out_size, 512) followed by ReLU\n",
    "        2) nn.Linear(512, n_actions)\n",
    "    - The final layer outputs one Q-value per possible action in the environment.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        c, h, w = input_shape # channels, height, width\n",
    "        \n",
    "        # Convolutional feature extractor\n",
    "        # ReLU after each layer ensures nonlinearity.\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # compute conv out size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c, h, w)\n",
    "            conv_out_size = int(np.prod(self.conv(dummy).shape[1:]))\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)            # pass input through convolutional feature extractor\n",
    "        x = x.view(x.size(0), -1)   # flatten spacial values into single vector\n",
    "        return self.fc(x)           # pass flattened values through fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292cd27",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "This is used to store experiences for the DQN to train on later on.<br>\n",
    "Storing the samples and sampling randomly them for training then:\n",
    "- Breaks correlation between consecutive samples\n",
    "- Stabilizes training\n",
    "- Allows re-using experiences multiple times (improves sample efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb758b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        # initialise empties\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity,), dtype=np.int64)\n",
    "        self.rewards = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.states[self.ptr] = state\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.dones[self.ptr] = float(done)\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # get random batch from buffer\n",
    "        idx = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return {\n",
    "            \"states\": self.states[idx],\n",
    "            \"actions\": self.actions[idx],\n",
    "            \"rewards\": self.rewards[idx],\n",
    "            \"next_states\": self.next_states[idx],\n",
    "            \"dones\": self.dones[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6356ae",
   "metadata": {},
   "source": [
    "## 4(c): Where the Q-Learning update is applied to the weights\n",
    "\n",
    "The train_step() function applies the Deep Q-Learning update by changing the weights of q_net, which is the main Q-network being trained. Its predictions are compared to target values that come from the separate target network.\n",
    "\n",
    "1) q_net is the network that actually learns. It produces the Q-values for the actions taken, and its weights are adjusted during training.\n",
    "2) target_net provides stable target values. It is a copy of q_net that is not updated during each training step, so it gives consistent targets.\n",
    "3) Only q_net is updated during the learning step. When backpropagation and the optimizer run, they modify the parameters of q_net only, while target_net remains unchanged until it is periodically refreshed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6eb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(q_net, target_net, optimizer, batch, gamma=0.99, device=\"cpu\"):\n",
    "    # convert all batch lists to tensors\n",
    "    states = torch.tensor(batch['states'], dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(batch['actions'], dtype=torch.long, device=device)\n",
    "    rewards = torch.tensor(batch['rewards'], dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor(batch['next_states'], dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(batch['dones'], dtype=torch.float32, device=device)\n",
    "\n",
    "    # compute q-values for current states\n",
    "    q_vals = q_net(states)\n",
    "    q_s_a = q_vals.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = target_net(next_states)                    # look at the next states\n",
    "        next_q_max = next_q.max(1)[0]                       # find the best next action\n",
    "        target = rewards + gamma * next_q_max * (1 - dones) # build the target value\n",
    "    \n",
    "    # Huber loss between predicted Q(s,a) and target value\n",
    "    loss = F.smooth_l1_loss(q_s_a, target)\n",
    "\n",
    "    # Standard optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping to avoid exploding updates\n",
    "    torch.nn.utils.clip_grad_norm_(q_net.parameters(), 10.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad8e6d",
   "metadata": {},
   "source": [
    "## Initialisation and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Environment\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\", repeat_action_probability=0.3) # keep actions for anti-jitter\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "# initial lives\n",
    "lives = info.get('lives', 5)\n",
    "\n",
    "# state stack init\n",
    "frame0 = preprocess_frame(obs)\n",
    "state_stack = np.stack([frame0]*4, axis=0)  # shape (4,84,84)\n",
    "state = state_stack.copy()\n",
    "\n",
    "# networks\n",
    "n_actions = env.action_space.n\n",
    "q_net = DQN(input_shape=(4,84,84), n_actions=n_actions).to(device)\n",
    "q_net.train()\n",
    "\n",
    "target_net = DQN(input_shape=(4,84,84), n_actions=n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# replay buffer\n",
    "replay_capacity = 10000\n",
    "buffer = ReplayBuffer(replay_capacity, state_shape=(4,84,84))\n",
    "\n",
    "# hyperparams\n",
    "batch_size = 32                 # size of batches of frames\n",
    "gamma = 0.80                    # how much the aget cares about future rewards\n",
    "initial_eps = 1.0               # initial epsilon before decay\n",
    "final_eps = 0.05                # final epsilon after decay\n",
    "eps_decay_steps = 1_000_00      # linear decay over this many steps\n",
    "start_train = 10000             # number of transitions before training starts\n",
    "train_freq = 4                  # train every n env steps\n",
    "target_update_freq = 10000      # sync target net every n env steps\n",
    "max_env_steps = 1_000_000       # total env steps to run\n",
    "save_path = \"dqn_breakout.pth\"  # path for saved model\n",
    "\n",
    "# rewards & punishments\n",
    "alignment_strength = 0.3\n",
    "anti_edge_strength = 0.1\n",
    "life_lost_penalty = 2.0\n",
    "anti_jitter_num = 0.1\n",
    "\n",
    "# bookkeeping\n",
    "total_steps = 0\n",
    "episode = 0\n",
    "loss_history = []\n",
    "prev_paddle_center = None\n",
    "prev_action = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af968877",
   "metadata": {},
   "source": [
    "## Data Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e929a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_STEP_INTERVAL = 1000\n",
    "\n",
    "# open csv file\n",
    "episode_f = open(\"stats/episodes.csv\", \"w+\", newline=\"\")\n",
    "episode_writer = csv.writer(episode_f)\n",
    "episode_writer.writerow([\"episode\", \"env_return\", \"shaped_return\", \"length\", \"lives_lost\"])\n",
    "\n",
    "reward_f = open(\"stats/reward_components.csv\", \"w+\", newline=\"\")\n",
    "reward_writer = csv.writer(reward_f)\n",
    "reward_writer.writerow([\"step\", \"base_reward\", \"life_penalty\", \"alignment_reward\", \"edge_penalty\", \"jitter_reward\", \"total_reward\"])\n",
    "\n",
    "train_f = open(\"stats/training.csv\", \"w+\", newline=\"\")\n",
    "train_writer = csv.writer(train_f)\n",
    "train_writer.writerow([\"step\", \"loss\", \"q_mean\", \"q_max\"])\n",
    "\n",
    "# accumulators\n",
    "step_acc = {\n",
    "    \"base_reward\": 0.0,\n",
    "    \"life_penalty\": 0.0,\n",
    "    \"alignment_reward\": 0.0,\n",
    "    \"edge_penalty\": 0.0,\n",
    "    \"jitter_reward\": 0.0,\n",
    "    \"total_reward\": 0.0,\n",
    "    \"count\": 0\n",
    "}\n",
    "\n",
    "# episode stats\n",
    "ep_env_return = 0.0\n",
    "ep_shaped_return = 0.0\n",
    "ep_len = 0\n",
    "ep_lives_lost = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25003b",
   "metadata": {},
   "source": [
    "## Reward Shaping\n",
    "1) Base Reward:\n",
    "    - The agent receives the environment's default reward from Gym, which includes points for breaking a block.\n",
    "2) Modification based on game features:\n",
    "    - Multiply base reward: Rewards from breaking bricks are scaled (3x) to emphasie their importance.\n",
    "    - Life loss penalty: If the agent loses lives, a pentaly is given to discourage risky actions.\n",
    "    - Alignment Reward: The closer the paddle is to the ball's x-position, the higher the reward.\n",
    "    - Anti edge-hugging penalty: The agent is discouraged from staying near the edges of the screen.\n",
    "    - Anti-jitter smoothing: Small rewards and penalties to keep action consistency to reduce paddle jitter.\n",
    "3) Purpose:\n",
    "    - These modifications move the agent towards desirable behaviours beyond just breaking bricks.\n",
    "    - They speed up learning by providing more freqeunt and informative feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c20a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_interval = 3\n",
    "last_display = 0\n",
    "\n",
    "PADDLE_WIDTH = 16\n",
    "PADDLE_Y = 193  \n",
    "SCREEN_CENTER = 128\n",
    "\n",
    "while total_steps < max_env_steps:\n",
    "    # epsilon linear decay\n",
    "    eps = max(final_eps, initial_eps - (initial_eps - final_eps) * (total_steps / eps_decay_steps))\n",
    "\n",
    "    # choose action\n",
    "    if random.random() < eps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            st = torch.tensor(state.astype(np.float32), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            qvals = q_net(st).cpu().numpy()[0]\n",
    "            action = int(np.argmax(qvals))\n",
    "\n",
    "    # step env\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_steps += 1\n",
    "\n",
    "    # data for logging\n",
    "    raw_reward = reward\n",
    "    ep_env_return += raw_reward\n",
    "\n",
    "    # multiply base block reward\n",
    "    reward *= 3\n",
    "\n",
    "    # detect life loss and apply penalty\n",
    "    life_penalty = 0.0\n",
    "    new_lives = info.get('lives', lives)\n",
    "    life_lost = new_lives < lives\n",
    "    lives = new_lives\n",
    "    if life_lost:\n",
    "        life_penalty = -life_lost_penalty\n",
    "        reward += life_penalty\n",
    "        ep_lives_lost += 1\n",
    "\n",
    "    # access ram values\n",
    "    ram = env.unwrapped.ale.getRAM()\n",
    "    ball_x = ram[99]\n",
    "    ball_y = ram[101]\n",
    "    paddle_x = ram[72]\n",
    "    paddle_center = paddle_x + 16/2\n",
    "    max_dist = SCREEN_CENTER\n",
    "    \n",
    "    # Alignemnt reward\n",
    "    alignment_error = abs(paddle_center - ball_x)\n",
    "    alignment_reward = (255 - alignment_error) / 255.0\n",
    "    reward += alignment_reward * alignment_strength\n",
    "\n",
    "    # anti edge-hugging\n",
    "    dist_from_center = abs(paddle_center - SCREEN_CENTER)\n",
    "    if abs(ball_y - PADDLE_Y) > 20:   # adjust threshold depending on RAM scale\n",
    "        dist_from_center = abs(paddle_center - SCREEN_CENTER)\n",
    "        edge_penalty = dist_from_center / max_dist\n",
    "        reward -= edge_penalty * anti_edge_strength\n",
    "\n",
    "    # anti jitter\n",
    "    jitter_reward = 0.0\n",
    "    if prev_action is not None:\n",
    "        jitter_reward = anti_jitter_num if action == prev_action else -anti_jitter_num\n",
    "        reward += jitter_reward\n",
    "    prev_action = action\n",
    "\n",
    "    # keep track of shaped reward\n",
    "    ep_shaped_return += reward\n",
    "    ep_len += 1\n",
    "\n",
    "    # rewards for step\n",
    "    step_acc[\"base_reward\"] += raw_reward\n",
    "    step_acc[\"life_penalty\"] += life_penalty\n",
    "    step_acc[\"alignment_reward\"] += alignment_reward\n",
    "    step_acc[\"edge_penalty\"] += edge_penalty\n",
    "    step_acc[\"jitter_reward\"] += jitter_reward\n",
    "    step_acc[\"total_reward\"] += reward\n",
    "    step_acc[\"count\"] += 1\n",
    "\n",
    "    # preprocess next state\n",
    "    next_frame = preprocess_frame(obs)\n",
    "    next_state = np.roll(state, shift=-1, axis=0)\n",
    "    next_state[-1] = next_frame\n",
    "\n",
    "    # training 'done' flag\n",
    "    done_for_training = bool(terminated or truncated)\n",
    "\n",
    "    # push to replay buffer\n",
    "    buffer.push(state.copy(), action, reward, next_state.copy(), done_for_training)\n",
    "\n",
    "    # advance current state\n",
    "    state = next_state\n",
    "\n",
    "    # If the actual episode ended then reset environment and bookkeeping\n",
    "    if terminated or truncated:\n",
    "        # log episode stats\n",
    "        episode_writer.writerow([\n",
    "            episode,\n",
    "            ep_env_return,\n",
    "            ep_shaped_return,\n",
    "            ep_len,\n",
    "            ep_lives_lost\n",
    "        ])\n",
    "        episode_f.flush()\n",
    "\n",
    "        # reset episode accumulators\n",
    "        ep_env_return = 0.0\n",
    "        ep_shaped_return = 0.0\n",
    "        ep_len = 0\n",
    "        ep_lives_lost = 0\n",
    "\n",
    "        # reset env\n",
    "        obs, info = env.reset()\n",
    "        lives = info.get('lives', 5)\n",
    "        frame0 = preprocess_frame(obs)\n",
    "        state = np.stack([frame0]*4, axis=0)\n",
    "        episode += 1\n",
    "\n",
    "    # Training step\n",
    "    if len(buffer) >= start_train and (total_steps % train_freq == 0):\n",
    "        batch = buffer.sample(batch_size)\n",
    "        loss = train_step(q_net, target_net, optimizer, batch, gamma=gamma, device=device)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # get batch states and log\n",
    "        with torch.no_grad():\n",
    "            states_t = torch.tensor(\n",
    "                batch[\"states\"],\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            )\n",
    "            q_vals = q_net(states_t)          # [B, num_actions]\n",
    "            q_mean = q_vals.mean().item()\n",
    "            q_max = q_vals.max().item()\n",
    "\n",
    "        train_writer.writerow([\n",
    "            total_steps,\n",
    "            loss,\n",
    "            q_mean,\n",
    "            q_max\n",
    "        ])\n",
    "        train_f.flush()\n",
    "\n",
    "    # Update target network periodically\n",
    "    if total_steps % target_update_freq == 0 and total_steps > 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Periodic inline display (less flicker)\n",
    "    if total_steps - last_display >= display_interval:\n",
    "        # compute q-values for overlay\n",
    "        with torch.no_grad():\n",
    "            st = torch.tensor(state.astype(np.float32), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            qvals = q_net(st).cpu().numpy()[0]\n",
    "        top3 = np.sort(qvals)[-3:][::-1]\n",
    "        show_frame_inline(obs, paddle_x, reward=reward, action=action, q_values=top3, scale=4, font_size=18)\n",
    "        last_display = total_steps\n",
    "\n",
    "    # Periodic \n",
    "    if total_steps % LOG_STEP_INTERVAL == 0 and step_acc[\"count\"] > 0:\n",
    "        c = step_acc[\"count\"]\n",
    "        reward_writer.writerow([\n",
    "            total_steps,\n",
    "            step_acc[\"base_reward\"] / c,\n",
    "            step_acc[\"life_penalty\"] / c,\n",
    "            step_acc[\"alignment_reward\"] / c,\n",
    "            step_acc[\"edge_penalty\"] / c,\n",
    "            step_acc[\"jitter_reward\"] / c,\n",
    "            step_acc[\"total_reward\"] / c,\n",
    "        ])\n",
    "        reward_f.flush()\n",
    "\n",
    "        for k in step_acc:\n",
    "            if k != \"count\":\n",
    "                step_acc[k] = 0.0\n",
    "        step_acc[\"count\"] = 0\n",
    "\n",
    "    # occasional checkpoint save\n",
    "    if total_steps % 50000 == 0 and total_steps > 0:\n",
    "        torch.save({\n",
    "            \"q_net\": q_net.state_dict(),\n",
    "            \"target_net\": target_net.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"total_steps\": total_steps\n",
    "        }, save_path)\n",
    "\n",
    "# end loop and close files\n",
    "env.close()\n",
    "episode_f.close()\n",
    "reward_f.close()\n",
    "train_f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc4edf",
   "metadata": {},
   "source": [
    "## Preview and Debugging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_shape = (4, 84, 84)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "model = DQN(input_shape, n_actions).to(device)\n",
    "checkpoint = torch.load(\"dqn_breakout.pth\")\n",
    "model.load_state_dict(checkpoint[\"q_net\"])\n",
    "model.eval()\n",
    "\n",
    "frame = preprocess_frame(obs)\n",
    "state = np.stack([frame]*4, axis=0)\n",
    "\n",
    "while True:\n",
    "    # Convert state to tensor\n",
    "    s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Greedy action (no randomness)\n",
    "    with torch.no_grad():\n",
    "        q = model(s)\n",
    "        action = int(torch.argmax(q))\n",
    "\n",
    "    # Step environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Preprocess new frame\n",
    "    next_frame = preprocess_frame(obs)\n",
    "    next_state = np.roll(state, -1, axis=0)\n",
    "    next_state[-1] = next_frame\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "        frame = preprocess_frame(obs)\n",
    "        state = np.stack([frame]*4, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a97de",
   "metadata": {},
   "source": [
    "#TODO plots, evaluations, references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
